# Project description:

In this project, we defined fact and dimension tables for a star schema for a particular analytic focus, and wrote an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

# Database design: 

The database is divided into fact and dimension tables. These tables are described below.

1. Fact Table
   songplays- records in log data associated with song plays i.e. records with page NextSong
              songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent 

2. Dimension Tables
   users-    users in the app with columns:
             user_id, first_name, last_name, gender, level
            
   songs-    songs in music database with columns:
             song_id, title, artist_id, year, duration
            
   artists-  artists in music database with columns:
             artist_id, name, location, latitude, longitude
              
   time -    timestamps of records in songplays broken down into specific units with columns:
             start_time, hour, day, week, month, year, weekday



# ETL Process:

## Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

1. song_data/A/B/C/TRABCEI128F424C983.json

2. song_data/A/A/B/TRAABJL12903CDCF1A.json

For example, TRAABJL12903CDCF1A.json, looks like:

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
Project Repository files: 

## Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

1. log_data/2018/11/2018-11-12-events.json

2. log_data/2018/11/2018-11-13-events.json

For example, 2018-11-12-events.json, looks like.

![Log Data!](/log-data.png "Log Data")

# How To Run the Project:

Following describes each file that the project has and its purpose:

1. test.ipynb displays the first few rows of each table to check database.

2. create_tables.py drops and creates tables. This file is used to reset tables before each time we run your ETL scripts.

3. etl.ipynb reads and processes a single file from song_data and log_data and loads the data into our tables.

4. etl.py reads and processes files from song_data and log_data and loads them into our tables.

5. sql_queries.py contains all our sql queries, and is imported into the last three files above.

6. README.md provides discussion on your project.

## Create Tables

1. Write CREATE statements in sql_queries.py to create each table.
2. Write DROP statements in sql_queries.py to drop each table if it exists.
3. Run create_tables.py to create your database and tables.
4. Run test.ipynb to confirm the creation of your tables with the correct columns.

## Build ETL Processes
Follow instructions in the etl.ipynb notebook to develop ETL processes for each table. At the end of each table section, or at the end of the notebook, run test.ipynb to confirm that records were successfully inserted into each table. Remember to rerun create_tables.py to reset your tables before each time you run this notebook.

## Build ETL Pipeline
Use what you've completed in etl.ipynb to complete etl.py, where you'll process the entire datasets. Remember to run create_tables.py before running etl.py to reset your tables. Run test.ipynb to confirm your records were successfully inserted into each table.

## Run Sanity Tests
When you are satisfied with your work, run the cell under the Sanity Tests section in the test.ipynb notebook. The cells contain some basic tests that will evaluate your work and catch any silly mistakes. We test column data types, primary key constraints and not-null constraints as well look for on-conflict clauses wherever required